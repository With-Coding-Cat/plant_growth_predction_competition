{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "개발 환경\n",
    "\n",
    "DISTRIB_ID=Ubuntu\n",
    "\n",
    "DISTRIB_RELEASE=18.04\n",
    "\n",
    "DISTRIB_CODENAME=bionic\n",
    "\n",
    "DISTRIB_DESCRIPTION=\"Ubuntu 18.04.5 LTS\"\n",
    "\n",
    "라이브러리 버전\n",
    "\n",
    "albumentations==1.1.0\n",
    "\n",
    "opencv-python==4.5.3.56\n",
    "\n",
    "glob2==0.7\n",
    "\n",
    "Pillow==8.3.2\n",
    "\n",
    "tensorboard == 2.7.0\n",
    "\n",
    "timm==0.4.12\n",
    "\n",
    "torch==1.8.1\n",
    "\n",
    "tqdm==5.62.3"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 필요 파일 설치 부분\n",
    "# !pip install glob2\n",
    "# !pip install opencv-python\n",
    "# !pip install albumentations\n",
    "# !pip install Pillow\n",
    "# !pip install tensorboard\n",
    "# !pip install timm\n",
    "# !pip install tqdm"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "먼저 필요한 라이브러리와 프레임워크를 미리 불러들이겠습니다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from itertools import combinations\n",
    "from PIL import Image\n",
    "\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import glob\n",
    "import cv2\n",
    "import shutil\n",
    "import tqdm\n",
    "import albumentations\n",
    "import albumentations.pytorch as albu_torch\n",
    "import torch\n",
    "import torch.nn as nn\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "우선, 훈련용 데이터 셋과 검증용 테스트 셋을 나누고, 검증용 데이터 셋과 테스트 셋의 경우 추론에 적합하도록 이미지 크기를 줄이도록 하겠습니다.\n",
    "\n",
    "이미지 각각의 크기가 매우 커서 이미지 로드와 리사이즈에 매우 시간이 많이 걸리는데, 불필요한 시간을 줄이기 위함입니다.\n",
    "\n",
    "다만, 추후 어그멘테이션 방법 때문에 훈련용 데이터 셋의 경우 리사이즈를 하지 않고 저장합니다.\n",
    "\n",
    "그리고 대회 참여를 위해 사용해야하는 test_data.csv 파일이 test_dataset에 있습니다만, glob을 통해서 쉽게 이미지를 추출하기 위해 빼도록 폴더 밖으로 빼도록 하겠습니다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# this root is important as the DACON require this part. but if you unzip the data file(.zip), the name is \"open\", just change this path.\n",
    "BASE_DATA_ROOT = '/data'\n",
    "# default folder name\n",
    "TEST_DATA_ROOT = 'test_dataset'\n",
    "TRAIN_VAL_DATA_ROOT = '/train_dataset'\n",
    "# validation set\n",
    "VAL_DATA_FOLDERS = ('BC_09', 'LT_10')\n",
    "TEST_CSV_NAME = 'test_data.csv'\n",
    "shutil.move(os.path.join(BASE_DATA_ROOT, TEST_DATA_ROOT, TEST_CSV_NAME), os.path.join(BASE_DATA_ROOT, TEST_CSV_NAME))\n",
    "# resize test set files all\n",
    "test_set_files = glob.glob(os.path.join(BASE_DATA_ROOT, TEST_DATA_ROOT, '*', '*', '*'))\n",
    "for file_path in tqdm.tqdm(test_set_files):\n",
    "    img = Image.open(file_path)\n",
    "    img = img.resize((448, 448))\n",
    "    img.save(file_path)\n",
    "\n",
    "# resize validation set files only\n",
    "train_val_set_files = glob.glob(os.path.join(BASE_DATA_ROOT, TRAIN_VAL_DATA_ROOT, '*', '*', '*'))\n",
    "for file_path in tqdm.tqdm(train_val_set_files):\n",
    "    for val_data_folder_name in VAL_DATA_FOLDERS:\n",
    "        if val_data_folder_name in file_path:\n",
    "            img = Image.open(file_path)\n",
    "            img = img.resize((448, 448))\n",
    "            img.save(file_path)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "데이터 셋에 대한 준비는 이것으로 간단히 끝났습니다.\n",
    "\n",
    "모델 아키텍처를 구성해보도록 하겠습니다.\n",
    "\n",
    "timm 이라는 라이브러리를 통해서 이미 훈련된 모델을 불러들일 수 있습니다.\n",
    "\n",
    "저는 Big Transfer(https://arxiv.org/abs/1912.11370)라는 논문에서 소개된 모델을 이용하겠습니다. 다른 SWIN transformer와 같은 모델도 고려하였지만, 파라미터 튜닝까지 다 할 것을 고려할 때, 현실적으로 불가능하였습니다.그리고 당연히 모델이 크면 클수록 좋지만, GPU 성능의 한계 때문에 최대 크기를 사용하지는 못했습니다.\n",
    "\n",
    "모델 구성의 아이디어를 간략히 설명하자면, 우선 timm 라이브러리에서 제공하는 이미 훈련된 모델을 사용하였습니다. Big Tranfer라는 모델로, 전이학습을 위해서 다양한 모델구조와 방법을 시도했던 논문으로 기존에 300만장의 데이터에 pretrain된 후, 다시한번 이를 448x448 이미지 사이즈로 1000개의 카테고리에 다시한번 fine tuning한 모델을 사용하였습니다.\n",
    "\n",
    "모델 아키텍처를 구성할 때, 제가 생각하기에 사람의 성장 정도를 파악할 때 사람의 키나 골격, 얼굴 형태 등 다양한 요소를 종합하여 판단내리듯, 식물이 성장한 시간을 추측하기 위해서도 다양한 feature로부터 계산을 이끌어내야한다고 판단하였습니다.\n",
    "\n",
    "따라서 공통된 feature를 추출하기 위해서 단일의 pretrained 모델을 사용하되, 이후 다양한 features를 concat을 하여 이후 연산을 진행하고자 하였습니다. 또한 이떄도 여러 번의 연산을 거치면(깊이가 깊어지면) 정확도가 높아질 거라 예상하고 깊이를 추가하였습니다.\n",
    "\n",
    "마지막으로 pretrained 모델을 로드할 때, pretrained 당시에 사용했던 이미지 크기와 hearder의 weight를 그대로 사용하였습니다. 이는 훈련 시간을 축소시키면서도 기존에 1000가지의 category에 이미 학습되었다면 충분히 feature로 활용할 수 있을 거라 가정하였습니다.(컴퓨터 성능 문제인지 시간 압박이 상당했습니다.)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from timm.data.transforms_factory import create_transform\n",
    "from PIL import Image\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class PlantAgePrediction(nn.Module):\n",
    "    def __init__(self, drop_p=0.3):\n",
    "        super().__init__()\n",
    "        self.base_model = timm.create_model('resnetv2_152x2_bitm', pretrained=True, drop_rate=drop_p)\n",
    "        self.activation = nn.GELU()\n",
    "        \n",
    "        self.drop = nn.Dropout(drop_p)\n",
    "        self.fc1 = nn.Linear(2000, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 500)\n",
    "        self.fc3 = nn.Linear(500, 200)\n",
    "        self.fc4 = nn.Linear(200, 1)\n",
    "        \n",
    "    def forward(self, young, old):\n",
    "        young = self.drop(self.activation(self.base_model(young)))\n",
    "        old = self.drop(self.activation(self.base_model(old)))\n",
    "        \n",
    "        total = torch.cat((young, old), dim=1)\n",
    "        total = self.drop(self.activation(self.fc1(total)))\n",
    "        total = self.drop(self.activation(self.fc2(total)))\n",
    "        total = self.drop(self.activation(self.fc3(total)))\n",
    "        total = self.fc4(total)\n",
    "        \n",
    "        return total"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "이후 실제로 훈련하기 위해 이미지 어그멘테이션과 데이터 normalize, 데이터 로드를 위한 데이터 셋을 준비하겠습니다.\n",
    "\n",
    "이미지 어그멘테이션의 경우, 우선 rotation과 flip을 준 이후 다양한 크기로 crop하였고, 유일한 규칙은 아무리 작아도 식물의 한 부분은 보여야 한다는 점과 crop과 flip이 두 이미지 모두 동일해야한다는 것입니다.\n",
    "이는 동일한 대상에게 특징적인 feature, 예를 들어 잎사귀의 형태, 해상도가 높은 경우 잎맥의 모습, 색상 등, 사람에게 명확하지 않더라도 모델이 구분하고 그를 기준으로 식물이 성장한 시간을 추론할 수 있지 않을까 가설을 세웠기 떄문입니다. 또한 마찬가지로 동일한 대상의 성장을 추측하는 것이 의미있기에, 같은 대상 파일에서만 성장 수준을 비교하도록 하였습니다.\n",
    "\n",
    "이때, 대회측에서 설명한 데이터 투입 구조는 항상 before - after 의 시간적 순서를 따르게 되므로, 모델 학습시에도 이런 구조를 따르도록 하며, 데이터 셋 구성 또한 이를 따르도록 만들었습니다.\n",
    "normalize의 경우, 두 식물간의 시간차가 최대 42까지 나는 것으로 확인되었는데, 이러한 큰 값은 학습을 불안정하게 만들 수 있어 값을 축소 시키기 위한 방법으로 시행하였습니다. 특히, 결과값이 1인 경우는 매우 많으나, 결과값이 커질수록 데이터의 양이 매우 줄어드는 문제가 있었습니다. 따라서, min-max normalize를 진행하였습니다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# augmentation for validation and test\n",
    "def only_resize():\n",
    "    return albumentations.Compose([\n",
    "        albumentations.Resize(448, 448, always_apply=True),\n",
    "        albumentations.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5), always_apply=True),\n",
    "        albu_torch.transforms.ToTensorV2()\n",
    "    ], additional_targets={'young': 'image', 'old': 'image'})\n",
    "\n",
    "# augmentation for train\n",
    "def only_rotate():\n",
    "    return albumentations.Compose([\n",
    "        albumentations.Flip(p=0.5),\n",
    "        albumentations.Rotate(limit=180, always_apply=True),\n",
    "        albumentations.augmentations.crops.transforms.RandomResizedCrop(448, 448, scale=(0.2, 1), ratio=(1, 1), always_apply=True),\n",
    "        albumentations.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5), always_apply=True),\n",
    "        albu_torch.transforms.ToTensorV2()\n",
    "    ], additional_targets={'young': 'image', 'old': 'image'})\n",
    "\n",
    "def image_load_only(img_path):\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    return img\n",
    "\n",
    "def image_agmentation_apply(young, old, augmantation):\n",
    "    image_dict = {'image': young, 'old': old}\n",
    "    aug_dict = augmantation(**image_dict)\n",
    "    return aug_dict['image'], aug_dict['old']\n",
    "\n",
    "def denormalize(predicted, min_value, max_value):\n",
    "    output = predicted * (max_value - min_value) + min_value\n",
    "    return output"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "저의 경우는 데이터를 미리 다 불러들임으로써 반복적인 데이터 로드로 인한 부하를 줄였습니다. 다만, 충분한 RAM 이 없는 분을 위해 그때 그때 데이터를 불러들이는 데이터셋도 적어둡니다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class PlantDataset(Dataset):\n",
    "    def __init__(self, base_folder_path, train=True):\n",
    "        super().__init__()\n",
    "        kinds = os.listdir(base_folder_path) # BC, LT\n",
    "        self.data = []\n",
    "        preload_img = {}\n",
    "        \n",
    "        for kind in kinds:\n",
    "            target_plant_kind_number_path = os.path.join(base_folder_path, kind)\n",
    "            target_plant_kind_number_images = os.listdir(target_plant_kind_number_path)\n",
    "            for target_plant_number in target_plant_kind_number_images:\n",
    "                if train and target_plant_number not in VAL_DATA_FOLDERS:\n",
    "                    target_plant_kind_number_images_path = os.path.join(target_plant_kind_number_path, target_plant_number)\n",
    "                    target_plant_images = sorted(os.listdir(target_plant_kind_number_images_path))\n",
    "                    target_plant_images_pair = list(combinations(target_plant_images, 2))\n",
    "                    for img_pair in target_plant_images_pair:\n",
    "                        young, old = img_pair\n",
    "                        young_path = os.path.join(target_plant_kind_number_images_path, young)\n",
    "                        old_path = os.path.join(target_plant_kind_number_images_path, old)\n",
    "                        young = int(re.findall(r'[0-9]+', young)[-1])\n",
    "                        old = int(re.findall(r'[0-9]+', old)[-1])\n",
    "                        if young_path not in preload_img:\n",
    "                            preload_img[young_path] = image_load_only(young_path)\n",
    "                        if old_path not in preload_img:\n",
    "                            preload_img[old_path] = image_load_only(old_path)\n",
    "                        \n",
    "                        self.data.append({'data': [preload_img[young_path], preload_img[old_path]], 'label': old-young})\n",
    "                        \n",
    "                elif not train and target_plant_number in VAL_DATA_FOLDERS:\n",
    "                    target_plant_kind_number_images_path = os.path.join(target_plant_kind_number_path, target_plant_number)\n",
    "                    target_plant_images = sorted(os.listdir(target_plant_kind_number_images_path))\n",
    "                    target_plant_images_pair = list(combinations(target_plant_images, 2))\n",
    "                    for img_pair in target_plant_images_pair:\n",
    "                        young, old = img_pair\n",
    "                        young_path = os.path.join(target_plant_kind_number_images_path, young)\n",
    "                        old_path = os.path.join(target_plant_kind_number_images_path, old)\n",
    "                        young = int(re.findall(r'[0-9]+', young)[-1])\n",
    "                        old = int(re.findall(r'[0-9]+', old)[-1])\n",
    "                        if young_path not in preload_img:\n",
    "                            preload_img[young_path] = image_load_only(young_path)\n",
    "                        if old_path not in preload_img:\n",
    "                            preload_img[old_path] = image_load_only(old_path)\n",
    "                        \n",
    "                        self.data.append({'data': [preload_img[young_path], preload_img[old_path]], 'label': old-young})\n",
    "                \n",
    "        if train:\n",
    "            self.augmentation = only_rotate()\n",
    "        else:\n",
    "            self.augmentation = only_resize()\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        young_path, old_path = self.data[index]['data']\n",
    "        young_img, old_img = image_agmentation_apply(young_path, old_path, self.augmentation)\n",
    "        \n",
    "        return young_img, old_img, torch.tensor(self.data[index]['label'], dtype=torch.float).unsqueeze(-1)\n",
    "\n",
    "    def _minmax_scaling(self):\n",
    "        labels = [one_dict['label'] for one_dict in self.data]\n",
    "        min_value = min(labels)\n",
    "        max_value = max(labels)\n",
    "        print(f'min: {min_value}, max: {max_value}')\n",
    "        for one_dict in self.data:\n",
    "            one_dict['label'] = (one_dict['label'] - min_value) / (max_value - min_value)\n",
    "            \n",
    "        return min_value, max_value\n",
    "        \n",
    "    def _get_minmax_scaling(self, min_value, max_value):\n",
    "        for one_dict in self.data:\n",
    "            one_dict['label'] = (one_dict['label'] - min_value) / (max_value - min_value)\n",
    "        \n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# when there's not enough RAM\n",
    "# def image_load(img_path_young, img_path_old, augmantation):\n",
    "#     young = cv2.imread(img_path_young)\n",
    "#     young = cv2.cvtColor(young, cv2.COLOR_BGR2RGB)\n",
    "#     old = cv2.imread(img_path_old)\n",
    "#     old = cv2.cvtColor(old, cv2.COLOR_BGR2RGB)\n",
    "#     image_dict = {'image': young, 'old': old}\n",
    "#     aug_dict = augmantation(**image_dict)\n",
    "#     return aug_dict['image'], aug_dict['old']\n",
    "\n",
    "# class PlantDataset(Dataset):\n",
    "#     def __init__(self, base_folder_path, train=True):\n",
    "#         super().__init__()\n",
    "#         kinds = os.listdir(base_folder_path) # BC, LT\n",
    "#         self.data = []\n",
    "        \n",
    "#         for kind in kinds:\n",
    "#             target_plant_kind_number_path = os.path.join(base_folder_path, kind)\n",
    "#             target_plant_kind_number_images = os.listdir(target_plant_kind_number_path)\n",
    "#             for target_plant_number in target_plant_kind_number_images:\n",
    "#                 if train and target_plant_number not in VAL_DATA_FOLDERS:\n",
    "#                     target_plant_kind_number_images_path = os.path.join(target_plant_kind_number_path, target_plant_number)\n",
    "#                     target_plant_images = sorted(os.listdir(target_plant_kind_number_images_path))\n",
    "#                     target_plant_images_pair = list(combinations(target_plant_images, 2))\n",
    "#                     for img_pair in target_plant_images_pair:\n",
    "#                         young, old = img_pair\n",
    "#                         young_path = os.path.join(target_plant_kind_number_images_path, young)\n",
    "#                         old_path = os.path.join(target_plant_kind_number_images_path, old)\n",
    "#                         young = int(re.findall(r'[0-9]+', young)[-1])\n",
    "#                         old = int(re.findall(r'[0-9]+', old)[-1])\n",
    "#                         self.data.append({'data': [young_path, old_path], 'label': old-young})\n",
    "                        \n",
    "#                 elif not train and target_plant_number in VAL_DATA_FOLDERS:\n",
    "#                     target_plant_kind_number_images_path = os.path.join(target_plant_kind_number_path, target_plant_number)\n",
    "#                     target_plant_images = sorted(os.listdir(target_plant_kind_number_images_path))\n",
    "#                     target_plant_images_pair = list(combinations(target_plant_images, 2))\n",
    "#                     for img_pair in target_plant_images_pair:\n",
    "#                         young, old = img_pair\n",
    "#                         young_path = os.path.join(target_plant_kind_number_images_path, young)\n",
    "#                         old_path = os.path.join(target_plant_kind_number_images_path, old)\n",
    "#                         young = int(re.findall(r'[0-9]+', young)[-1])\n",
    "#                         old = int(re.findall(r'[0-9]+', old)[-1])\n",
    "#                         self.data.append({'data': [young_path, old_path], 'label': old-young})\n",
    "                \n",
    "#         if train:\n",
    "#             self.augmentation = only_rotate()\n",
    "#         else:\n",
    "#             self.augmentation = only_resize()\n",
    "            \n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "    \n",
    "#     def __getitem__(self, index):\n",
    "#         young_path, old_path = self.data[index]['data']\n",
    "#         young_img, old_img = image_load(young_path, old_path, self.augmentation)\n",
    "        \n",
    "#         return young_img, old_img, torch.tensor(self.data[index]['label'], dtype=torch.float).unsqueeze(-1)\n",
    "    \n",
    "#     def _minmax_scaling(self):\n",
    "#         labels = [one_dict['label'] for one_dict in self.data]\n",
    "#         min_value = min(labels)\n",
    "#         max_value = max(labels)\n",
    "#         print(f'min: {min_value}, max: {max_value}')\n",
    "#         for one_dict in self.data:\n",
    "#             one_dict['label'] = (one_dict['label'] - min_value) / (max_value - min_value)\n",
    "            \n",
    "#         return min_value, max_value\n",
    "        \n",
    "#     def _get_minmax_scaling(self, min_value, max_value):\n",
    "#         for one_dict in self.data:\n",
    "#             one_dict['label'] = (one_dict['label'] - min_value) / (max_value - min_value)\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "다음으로 실제 훈련을 위한 코드입니다.\n",
    "\n",
    "특징으로는 AdamW를 사용했고, 공식적으로는 RMSE를 기준으로 채점한다고 하였습니다만, 현재 준비된 데이터 셋이 normalize가 되었을 뿐만 아니라, 오차가 클 수 있는 데이터 예측치일 수록 데이터 수량이 적어진다는 것을 고려할 때, 오히려 이러한 불균형을 고려하는 것이 중요할 것으로 생각하였습니다.\n",
    "\n",
    "그러면 이러한 불균형에 좀더 가중치를 줄 수 있는 loss function이 필요할 것으로 생각되었고, 동시에 오히려 빈발하는 데이터 결과 값에 대해서는 낮은 가중치를 주는게 더 좋은 결과를 내지 않을까 생각하였습니다. 그 결과 저는 MSE를 loss function으로 사용하였습니다.\n",
    "\n",
    "그리고 tensorboard를 통해 실시간으로 관찰하였고, 과적합이 시사될 때 훈련을 중단하였습니다.\n",
    "\n",
    "사용법은 커맨드 창에서\n",
    "\n",
    "tensorboard --logdir=[폴더경로]\n",
    "\n",
    "이렇게 하면 되겠습니다. 아래 코드에서는 \"log_dir\"을 기본값으로 주었습니다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Total epochs is not important. I always checked the training graph, and if there's over fitting sign, I stopped.\n",
    "EPOCHS = 10000\n",
    "# This Batch size is not optimal. I want to try more large batch size, but my GPU.... sadly.... \n",
    "BATCH_SIZE = 12\n",
    "# Almost only hyperparameter that I can test. This was best in this setting.\n",
    "DROPOUT_RATE = 0.3\n",
    "LEARNING_RATE = 0.00001\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "LOG_DIR = \"log_dir\"\n",
    "DATASET_FOLDER = os.path.join(BASE_DATA_ROOT, TRAIN_VAL_DATA_ROOT)\n",
    "SAVE_FOLDER = 'save_model'\n",
    "os.makedirs(SAVE_FOLDER, exist_ok=True)\n",
    "\n",
    "print('Train data starts to load.')\n",
    "train_dataset = PlantDataset(DATASET_FOLDER, train=True)\n",
    "MINMAX_MIN, MINMAX_MAX = train_dataset._minmax_scaling()\n",
    "print('Test data starts to load.')\n",
    "val_dataset = PlantDataset(DATASET_FOLDER, train=False)\n",
    "val_dataset._get_minmax_scaling(MINMAX_MIN, MINMAX_MAX)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, prefetch_factor=BATCH_SIZE*2, pin_memory=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, prefetch_factor=BATCH_SIZE*2, pin_memory=True)\n",
    "\n",
    "print('Model starts to load.')\n",
    "model = PlantAgePrediction(drop_p=DROPOUT_RATE)\n",
    "model = torch.nn.DataParallel(model).to(DEVICE)\n",
    "print('Model is loaded.')\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# This is to continue training.\n",
    "# checkpoint = torch.load('save_model_temp/latest.pt', map_location=DEVICE)\n",
    "# for key in list(checkpoint.keys()):\n",
    "#     if 'module.' in key:\n",
    "#         checkpoint[key.replace('module.', '')] = checkpoint[key]\n",
    "#         del checkpoint[key]\n",
    "# model.load_state_dict(checkpoint['model'])\n",
    "# optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "# del checkpoint\n",
    "loss_fn = nn.MSELoss()\n",
    "writer = SummaryWriter(LOG_DIR)\n",
    "\n",
    "step = 0\n",
    "loss_min = float('inf')\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    for data in tqdm.tqdm(train_dataloader):\n",
    "        young, old, labels = data\n",
    "        young = young.to(DEVICE)\n",
    "        old = old.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(young, old)\n",
    "        loss = loss_fn(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        step += 1\n",
    "        writer.add_scalar('Loss/train_step', loss, step)\n",
    "    \n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm.tqdm(val_dataloader):\n",
    "            young, old, labels = data\n",
    "            young = young.to(DEVICE)\n",
    "            old = old.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            logits = model(young, old)\n",
    "            total_loss += loss_fn(logits, labels) * len(labels)\n",
    "            \n",
    "    if loss_min > total_loss:\n",
    "        loss_min = total_loss\n",
    "        torch.save(model.state_dict(), SAVE_FOLDER + '/min_loss.pt')\n",
    "    torch.save(model.state_dict(), SAVE_FOLDER + f'/{epoch}.pt')\n",
    "    torch.save({'model': model.state_dict(), 'optimizer': optimizer.state_dict()}, SAVE_FOLDER + f'/latest.pt')\n",
    "    writer.add_scalar('Loss/val', total_loss/len(val_dataset), epoch)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "마지막으로 충분히 훈련이 진행되었을 때, 테스트 셋을 불러와서 csv 에 결과를 산출하게 됩니다.\n",
    "\n",
    "이때, normalized 값은 산출하도록 훈련되었기에, 이를 원래 값으로 복원해야만 합니다.\n",
    "\n",
    "또한 후처리가 진행되는데, 모델이 산출하는 결과는 음수가 나올 수 있습니다만, csv 형식 상, 항상 데이터가 투입되는 구조는 before_after 형식을 따르게 되므로, 항상 0 이상의 값이며, 데이터 구조상 논리적으로 최저 1을 주는 것이 합당하다고 생각하여 1보다 작은 값이 산출될 경우 일괄적으로 1로 변환하였습니다.\n",
    "\n",
    "진행하기에 앞서, 데이터 셋 파일이 RAM을 매우 많이 차지하고 있기에 이를 제거해둡니다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "del train_dataset\n",
    "del val_dataset"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, img_path_for_glob, csv_path):\n",
    "        target_files = glob.glob(img_path_for_glob)\n",
    "        self.data = []\n",
    "        preload_img = {}\n",
    "        with open(csv_path, 'r', encoding='utf-8') as f:\n",
    "            csv_reader = csv.reader(f)\n",
    "            csv_reader = list(csv_reader)\n",
    "            for line in csv_reader[1:]:\n",
    "                _, young, old = line\n",
    "                for file_name in target_files:\n",
    "                    if young in file_name:\n",
    "                        young_path = file_name\n",
    "                    if old in file_name:\n",
    "                        old_path = file_name\n",
    "                if young_path not in preload_img:\n",
    "                    preload_img[young_path] = image_load_only(young_path)\n",
    "                if old_path not in preload_img:\n",
    "                    preload_img[old_path] = image_load_only(old_path)\n",
    "                self.data.append({'data': [preload_img[young_path], preload_img[old_path]]})\n",
    "            \n",
    "            self.augmentation = only_resize()\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        young_path, old_path = self.data[index]['data']\n",
    "        young_img, old_img = image_agmentation_apply(young_path, old_path, self.augmentation)\n",
    "        \n",
    "        return young_img, old_img\n",
    "    \n",
    "    \n",
    "# When there's not enough RAM\n",
    "# class TestDataset(Dataset):\n",
    "#     def __init__(self, img_path_for_glob, csv_path):\n",
    "#         target_files = glob.glob(img_path_for_glob)\n",
    "#         self.data = []\n",
    "#         with open(csv_path, 'r', encoding='utf-8') as f:\n",
    "#             csv_reader = csv.reader(f)\n",
    "#             csv_reader = list(csv_reader)\n",
    "#             for line in csv_reader[1:]:\n",
    "#                 _, young, old = line\n",
    "#                 for file_name in target_files:\n",
    "#                     if young in file_name:\n",
    "#                         young_path = file_name\n",
    "#                     if old in file_name:\n",
    "#                         old_path = file_name\n",
    "#                 self.data.append({'data': [young_path, old_path]})\n",
    "            \n",
    "#             self.augmentation = only_resize()\n",
    "            \n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "    \n",
    "#     def __getitem__(self, index):\n",
    "#         young_path, old_path = self.data[index]['data']\n",
    "#         young_img, old_img = image_load(young_path, old_path, self.augmentation)\n",
    "        \n",
    "#         return young_img, old_img"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "BATCH_SIZE = 1\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "TEST_CSV_NAME = 'test_data.csv'\n",
    "SAVE_FOLDER = 'save_model'\n",
    "CSV_SAVE_FOLDER = 'predicted_csv'\n",
    "os.makedirs(CSV_SAVE_FOLDER, exist_ok=True)\n",
    "\n",
    "test_dataset = TestDataset(img_path_for_glob=os.path.join(BASE_DATA_ROOT, TEST_DATA_ROOT, '*', '*', '*'), csv_path=os.path.join(BASE_DATA_ROOT, TEST_CSV_NAME))\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, prefetch_factor=BATCH_SIZE*2, pin_memory=True)\n",
    "\n",
    "model = PlantAgePrediction().to(DEVICE)\n",
    "for model_num in range(1, 1000): # the range function can be changed with tuple or list with numbers. These numbers are saved model weights file names.\n",
    "    MODEL_NUMBER = str(model_num)\n",
    "    checkpoint = torch.load(SAVE_FOLDER + '/' + MODEL_NUMBER + '.pt', map_location=DEVICE)\n",
    "\n",
    "    for key in list(checkpoint.keys()):\n",
    "        if 'module.' in key:\n",
    "            checkpoint[key.replace('module.', '')] = checkpoint[key]\n",
    "            del checkpoint[key]\n",
    "    model.load_state_dict(checkpoint)\n",
    "    del checkpoint\n",
    "    step = 0\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        with open(CSV_SAVE_FOLDER + '/' + MODEL_NUMBER + '.csv', 'w', encoding='utf-8') as f: # predicted results are saved CSV_SAVE_FOLDER.\n",
    "            csv_writer = csv.writer(f)\n",
    "            temp = ['idx','time_delta']\n",
    "            csv_writer.writerow(temp)\n",
    "            for data in tqdm.tqdm(test_dataloader):\n",
    "                young, old = data\n",
    "                young = young.to(DEVICE)\n",
    "                old = old.to(DEVICE)\n",
    "                logits = model(young, old)\n",
    "                temp = [step, max(1, denormalize(float(logits.detach().cpu().numpy()), MINMAX_MIN, MINMAX_MAX))]\n",
    "                csv_writer.writerow(temp)\n",
    "                step += 1\n",
    "            "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "이것으로 종료되었습니다.\n",
    "\n",
    "GPU의 한계와 개인적인 일들이 있어 시간을 많이 투자 할 수 없어서 아쉬움은 있습니다.\n",
    "\n",
    "가장 어려웠던 점은 validation 결과와 실제로 리더보드의 결과가 매우 불일치하는 경우들이 있었던 것입니다. 제가 제출한 결과 중 제일 성능이 좋았던 것 또한 해당 모델 훈련 과정 중 validation 결과 상에서 최고의 결과가 아니라 상위 4번째의 것이었습니다. validation set이 적기 때문에 벌어진 일이었을 수는 있으나 상당히 감을 잡기 난해하였습니다.\n",
    "\n",
    "공부가 제일 큰 목적이었기에, 모델 아키텍쳐 구성, 파라미터 튜닝, 데이터셋 처리 등에 초점을 두다보니 앙상블은 시도하지 못한게 아쉽네요. k-fold 형식을 도입했었어도 다소 향상이 있을 수 있지 않았을까 합니다.\n",
    "\n",
    "GPU 문제를 제외하고, 제가 생각했지만 시간 상 시도하지 못했던 것이 있습니다.\n",
    "\n",
    "현재 모델은 투입되는 이미지 크기의 20%(최소 식물 하나는 보일 수 있는 크기)부터 100%(전체 이미지)까지 다양한 크기로 crop하여 448*448 크기로 리사이즈하여 학습하게 되었습니다.\n",
    "\n",
    "다만,  원래 제가 세운 가설로는 하나의 식물이 지니는 나이를 유추할 수 있는 feature를 모델이 학습하고 이를 통해서 결과값을 산출 할 수 있다는 것이었고, 특히 하나의 이미지에 항상 식물이 5개가 있었으므로, 다양한 크기로 crop하여 학습하더라도 5개의 식물로부터 평균내어 정확한 결과값을 산출할 수 있을거라고 기대하였습니다.\n",
    "\n",
    "그런데 대회 하루 앞두고 들었던 생각으로, 근본적인 문제가 전체 이미지가 resize되면서 세부적인 이미지 정보가 사라질 수 있다는 점입니다. 그러면 제가 생각했던 가설이 근본부터 틀리게 되는 결과를 낳지 않았을까 합니다\n",
    "\n",
    "따라서 추후 제안해보고 싶은 것은, 이미지를 20%~40% 크기로 크롭하여 식물의 세밀한 feature를 모델이 충분히 학습할 수 있도록한 뒤, TEST 시점에서는 주어진 이미지를 4분할, 혹은 식물이 있는 곳을 중심으로 일부 중첩되더라도 5분할로 하여 이에 대한 결과값들을 평균내는 식으로 하면 정확도가 더 오를 수 있지 않았을까 합니다.\n",
    "\n",
    "아쉽지만 여기서 마무리 하겠습니다. 모두 수고하셨습니다."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.12 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "98b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}